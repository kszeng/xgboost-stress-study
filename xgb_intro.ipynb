{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53940</td>\n",
       "      <td>53940</td>\n",
       "      <td>53940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>SI1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>21551</td>\n",
       "      <td>11292</td>\n",
       "      <td>13065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          cut  color clarity\n",
       "count   53940  53940   53940\n",
       "unique      5      7       8\n",
       "top     Ideal      G     SI1\n",
       "freq    21551  11292   13065"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "diamonds.head()\n",
    "\n",
    "# in real-world datasets, need to explore, clean, and visualize the dataset first\n",
    "# here, 5-number summary of the numeric and categorial features built-in to seaborn\n",
    "diamonds.describe(exclude = np.number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Build an XGBoost DMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carat       float64\n",
      "cut        category\n",
      "color      category\n",
      "clarity    category\n",
      "depth       float64\n",
      "table       float64\n",
      "x           float64\n",
      "y           float64\n",
      "z           float64\n",
      "dtype: object\n",
      "3.0.0\n",
      "BUILTIN_PREFETCH_PRESENT: True\n",
      "CLANG_VERSION: [15, 0, 0]\n",
      "DEBUG: False\n",
      "MM_PREFETCH_PRESENT: False\n",
      "USE_CUDA: False\n",
      "USE_DLOPEN_NCCL: False\n",
      "USE_FEDERATED: False\n",
      "USE_NCCL: False\n",
      "USE_OPENMP: True\n",
      "USE_RMM: False\n",
      "libxgboost: /opt/anaconda3/lib/python3.12/site-packages/xgboost/lib/libxgboost.dylib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# goal: predict diamond prices using their physical measurements, so target will be the price column\n",
    "# candidate features are isolated into X and target labels into y\n",
    "\n",
    "# extract feature and target arrays\n",
    "X, y = diamonds.drop('price', axis=1), diamonds[['price']]\n",
    "\n",
    "# this dataset has three categorical columns. normally would encode with ordinal or one-hot encoding\n",
    "# XGBoost as the ability to internally deal with categoricals by casting to pandas \"category\" data type\n",
    "\n",
    "# extract text features\n",
    "cats = X.select_dtypes(exclude = np.number).columns.tolist()\n",
    "\n",
    "# convert to pandas category\n",
    "for col in cats:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "# should get three category features when printing dtypes attribute:\n",
    "print(X.dtypes)\n",
    "\n",
    "# split the data into train and test sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 1)\n",
    "\n",
    "# create regression matrices\n",
    "import xgboost as xgb\n",
    "print(xgb.__version__)\n",
    "build_info = xgb.build_info()\n",
    "for name in sorted(build_info.keys()):\n",
    "    print(f'{name}: {build_info[name]}')\n",
    "\n",
    "dtrain_reg = xgb.DMatrix(X_train, y_train, enable_categorical = True)\n",
    "dtest_reg = xgb.DMatrix(X_test, y_test, enable_categorical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python XGBoost Regression\n",
    "\n",
    "**After building the DMatrices, need to choose a value for the `objective` parameter. This tells XGBoost the machine learning problem to be solved and what metrics or loss functions to use to solve that problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The chosen objective function and any other hyperparameters of XGBoost should be specified in a dictionary, which by convention should be called params.\n",
    "\n",
    "Inside these initial `params`, also set `tree_method` to `gpu_hist`, which enables GPU acceleration. If no GPU, can omit the parameter or set it to `hist`.\n",
    "\n",
    "Then, set another parameter called `num_boost_round`, which stands for number of boosting rounds. Internally, XGBoost minimizes the loss function RMSE in small incremental rounds; this parameter specifies the number of those rounds.\n",
    "\n",
    "Ideal number of rounds is usually found through hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "params = {'objective': 'reg:squarederror', \n",
    "          'tree_method': 'hist'} # set tree_method to hist because no GPU\n",
    "\n",
    "n = 100 \n",
    "model = xgb.train(\n",
    "    params = params,\n",
    "    dtrain = dtrain_reg, \n",
    "    num_boost_round = n,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "1. Use `dtest_reg` DMatrix to measure the model's performance on unseen data.\n",
    "2. Once predictions are generated with `predict`, pass them to `mean_squared_error` function of Sklearn to compare against `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the base bodel: 552.861\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "preds = model.predict(dtest_reg)\n",
    "\n",
    "# compare results against y_test\n",
    "rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "print(f\"RMSE of the base bodel: {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Validation Sets During Training\n",
    "\n",
    "Use evaluation arrays to track model performance in real time as it improves incrementally across boosting rounds.\n",
    "\n",
    "1. Set up parameters again.\n",
    "2. Create a list of two tuples that each contain: array for the model to evaluate; array name.\n",
    "3. Pass array to `evals` parameter of `xgb.train` and see model performance after each boosting round.\n",
    "\n",
    "**Notes:** when using high number of boosting rounds, can use `verbose_eval` parameter to print output every `verbose_eval` rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2874.49146\tvalidation-rmse:2817.90814\n",
      "[10]\ttrain-rmse:548.36512\tvalidation-rmse:592.03160\n",
      "[20]\ttrain-rmse:491.09887\tvalidation-rmse:558.53485\n",
      "[30]\ttrain-rmse:469.58201\tvalidation-rmse:555.51015\n",
      "[40]\ttrain-rmse:454.32953\tvalidation-rmse:554.45666\n",
      "[50]\ttrain-rmse:438.68033\tvalidation-rmse:554.13365\n",
      "[60]\ttrain-rmse:425.38361\tvalidation-rmse:551.57888\n",
      "[70]\ttrain-rmse:414.71115\tvalidation-rmse:549.26109\n",
      "[80]\ttrain-rmse:405.41008\tvalidation-rmse:549.03952\n",
      "[90]\ttrain-rmse:391.04269\tvalidation-rmse:551.87206\n",
      "[99]\ttrain-rmse:383.48826\tvalidation-rmse:552.86131\n"
     ]
    }
   ],
   "source": [
    "# 1: set up params\n",
    "params = {\"objective\": \"reg:squarederror\",\n",
    "          \"tree_method\": \"hist\"}\n",
    "n = 100\n",
    "\n",
    "# 2: set array and array names\n",
    "evals = [(dtrain_reg, \"train\"), (dtest_reg, \"validation\")]\n",
    "\n",
    "# 3: track model performance\n",
    "model = xgb.train(\n",
    "    params = params,\n",
    "    dtrain = dtrain_reg, \n",
    "    num_boost_round = n,\n",
    "    evals = evals,\n",
    "    verbose_eval = 10 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Early Stopping\n",
    "\n",
    "Goal: achieve **golden middle**, where the model has learned enough to optimize performance on the validation set. Can use **early stopping** to force the model to stop when validation loss achieves stable, optimized value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:2874.49146\tvalidation-rmse:2817.90814\n",
      "[50]\ttrain-rmse:438.68033\tvalidation-rmse:554.13365\n",
      "[100]\ttrain-rmse:381.96310\tvalidation-rmse:553.73941\n",
      "[128]\ttrain-rmse:358.11000\tvalidation-rmse:553.05030\n"
     ]
    }
   ],
   "source": [
    "n = 10000\n",
    "\n",
    "model = xgb.train(\n",
    "    params = params,\n",
    "    dtrain = dtrain_reg, \n",
    "    num_boost_round = n,\n",
    "    evals = evals,\n",
    "    verbose_eval = 50, \n",
    "    early_stopping_rounds = 50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Cross-Validation\n",
    "\n",
    "**k-fold cross-validation:** set aside a test set for final performance evaluation of each model. Split training data into $k$ folds. Use $k - 1$ segments for training and the $k$th part for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550.2735543625861"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\"objective\": \"reg:squarederror\",\n",
    "          \"tree_method\": \"hist\"}\n",
    "n = 1000\n",
    "\n",
    "results = xgb.cv(\n",
    "    params, dtrain_reg,\n",
    "    num_boost_round = n,\n",
    "    nfold = 5, \n",
    "    early_stopping_rounds = 20\n",
    ")\n",
    "\n",
    "results.head()\n",
    "\n",
    "# take the minimum of the test-rmse-mean column\n",
    "best_rmse = results['test-rmse-mean'].min()\n",
    "\n",
    "best_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Classification\n",
    "\n",
    "Two most popular classification objectives:\n",
    "1. `binary:logistic`: binary classification (the target contains only two classes)\n",
    "2. `multi:softprob`: multi-class classification (more than two classes in the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "X, y = diamonds.drop(\"cut\", axis=1), diamonds[[\"cut\"]]\n",
    "\n",
    "# encode y to numeric\n",
    "y_encoded = OrdinalEncoder().fit_transform(y)\n",
    "\n",
    "# extract text features\n",
    "cats = X.select_dtypes(exclude = np.number).columns.tolist()\n",
    "\n",
    "# convert to pd.Categorical\n",
    "for col in cats:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "# split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, random_state=1, stratify=y_encoded)\n",
    "\n",
    "# create classification matrices\n",
    "dtrain_clf = xgb.DMatrix(X_train, y_train, enable_categorical = True)\n",
    "dtest_clf = xgb.DMatrix(X_test, y_test, enable_categorical = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set params\n",
    "params = {'objective': 'multi:softprob',\n",
    "          'tree_method': 'hist',\n",
    "          'num_class': 5}\n",
    "n = 1000\n",
    "\n",
    "# train the model using 5-fold cv\n",
    "results = xgb.cv(\n",
    "    params, dtrain_clf,\n",
    "    num_boost_round = n, \n",
    "    nfold = 5,\n",
    "    metrics = {'mlogloss', 'auc', 'merror'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9403143599245043"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# three classification metrics were used. results:\n",
    "results.keys()\n",
    "\n",
    "# see the best AUC score (take the maximum of test-auc-mean column)\n",
    "results['test-auc-mean'].max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
